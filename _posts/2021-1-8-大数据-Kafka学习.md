---
title: Kafka学习
date: 2021-1-8 23:29:53
categories:
- 大数据
tags:
- 大数据
---

## 1、Kafka简介

### 1.1、什么是Kafka?

​    Kafka 是由Linkedin公司开发的，它是一个**分布式**的，支持**多分区**、**多副本**，基于 Zookeeper 的分布式消息流平台，它同时也是一款开源的**基于发布订阅模式的消息引擎系统**。 

​       一个典型的 Kafka 集群中包含**若干Producer（可以是web前端产生的Page View，或者是服务器日志，系统CPU、Memory等），若干broker（Kafka支持水平扩展，一般broker数量越多，集群吞吐率越高），若干Consumer Group，以及一个Zookeeper集群**。Kafka通过Zookeeper管理集群配置，选举leader，以及在Consumer Group发生变化时进行rebalance。Producer使用push模式将消息发布到broker，Consumer使用pull模式从broker订阅并消费消息。 

![]({{ site.url }}/assets/img/大数据/1.1.jpeg)


### 1.2、Kafka的特性

- `高吞吐、低延迟`：kakfa 最大的特点就是收发消息非常快，kafka 每秒可以处理几十万条消息，它的最低延迟只有几毫秒。
- `高伸缩性`：每个主题(topic) 包含多个分区(partition)，主题中的分区可以分布在不同的主机(broker)中。
- `持久性、可靠性`：Kafka 能够允许数据的持久化存储，消息被持久化到磁盘，并支持数据备份防止数据丢失，Kafka 底层的数据存储是基于 Zookeeper 存储的，Zookeeper 我们知道它的数据能够持久存储。
- `容错性`：允许集群中的节点失败，某个节点宕机，Kafka 集群能够正常工作
- `高并发`：支持数千个客户端同时读写

### 1.3、Kafka的应用场景

- **活动跟踪**：Kafka 可以用来跟踪用户行为，比如我们经常回去淘宝购物，你打开淘宝的那一刻，你的登陆信息，登陆次数都会作为消息传输到 Kafka ，当你浏览购物的时候，你的浏览信息，你的搜索指数，你的购物爱好都会作为一个个消息传递给 Kafka ，这样就可以生成报告，可以做智能推荐，购买喜好等。
- **传递消息**：Kafka 另外一个基本用途是传递消息，应用程序向用户发送通知就是通过传递消息来实现的，这些应用组件可以生成消息，而不需要关心消息的格式，也不需要关心消息是如何发送的。
- **度量指标**：Kafka也经常用来记录运营监控数据。包括收集各种分布式应用的数据，生产各种操作的集中反馈，比如报警和报告。
- **日志记录**：Kafka 的基本概念来源于提交日志，比如我们可以把数据库的更新发送到 Kafka 上，用来记录数据库的更新时间，通过kafka以统一接口服务的方式开放给各种consumer，例如hadoop、Hbase、Solr等。
- **流式处理**：流式处理是有一个能够提供多种应用程序的领域。
- **限流削峰**：Kafka 多用于互联网领域某一时刻请求特别多的情况下，可以把请求写入Kafka 中，避免直接请求后端程序导致服务崩溃。

## 2、生产者和消费者

​    对于 Kafka 来说客户端有两种基本类型：**生产者（Producer）**和**消费者（Consumer）**。这很容易理解，生产者（也称为发布者）创建消息，而消费者（也称为订阅者）负责消费or读取消息。Kafka 有四个核心API，它们分别是

- Producer API：它允许应用程序向一个或多个 topics 上发送消息记录
- Consumer API：允许应用程序订阅一个或多个 topics 并处理为其生成的记录流
- Streams API：它允许应用程序作为流处理器，从一个或多个主题中消费输入流并为其生成输出流，有效的将输入流转换为输出流。
- Connector API：它允许构建和运行将 Kafka 主题连接到现有应用程序或数据系统的可用生产者和消费者。例如，关系数据库的连接器可能会捕获对表的所有更改

![]({{ site.url }}/assets/img/大数据/1.2.png)


### 2.1、生产者概述

​    在 Kafka 中，我们把产生消息的那一方称为`生产者`，比如我们经常回去淘宝购物，你打开淘宝的那一刻，你的登陆信息，登陆次数都会作为消息传输到 Kafka 后台，当你浏览购物的时候，你的浏览信息，你的搜索指数，你的购物爱好都会作为一个个消息传递给 Kafka 后台，然后淘宝会根据你的爱好做智能推荐，致使你的钱包从来都禁不住诱惑，那么这些生产者产生的`消息`是怎么传到 Kafka 应用程序的呢？发送过程是怎么样的呢？

尽管消息的产生非常简单，但是消息的发送过程还是比较复杂的，下图是一个 Kafka Producer 发送消息的过程图，先大致理解一下，下面我们会进行详细的介绍。

![]({{ site.url }}/assets/img/大数据/1.3.png)


​     我们从创建一个`ProducerRecord` 对象开始，ProducerRecord 是 Kafka 中的一个核心类，它代表了一组 Kafka 需要发送的 key/value键值对，它由记录要发送到的**主题名称**（Topic Name），**可选的分区号**（Partition Number）以及**可选的键值对**构成。

​    在发送 ProducerRecord 时，我们需要将键值对对象由序列化器转换为字节数组，这样它们才能够在网络上传输。然后消息到达了分区器。

​    如果发送过程中指定了有效的分区号，那么在发送记录时将使用该分区。如果发送过程中未指定分区，则将使用key 的 hash 函数映射指定一个分区。如果发送的过程中既没有分区号也没有，则将以循环的方式分配一个分区。选好分区后，生产者就知道向哪个主题和分区发送数据了。

​    ProducerRecord 还有关联的时间戳，如果用户没有提供时间戳，那么生产者将会在记录中使用当前的时间作为时间戳。Kafka 最终使用的时间戳取决于 topic 主题配置的时间戳类型。

- 如果将主题配置为使用 `CreateTime`，则生产者记录中的时间戳将由 broker 使用。
- 如果将主题配置为使用`LogAppendTime`，则生产者记录中的时间戳在将消息添加到其日志中时，将由 broker 重写。

​    然后，这条消息被存放在一个记录批次里，这个批次里的所有消息会被发送到相同的主题和分区上。由一个独立的线程负责把它们发到 Kafka Broker 上。

​    Kafka Broker 在收到消息时会返回一个响应，如果写入成功，会返回一个 RecordMetaData 对象，**它包含了主题和分区信息，以及记录在分区里的偏移量，上面两种的时间戳类型也会返回给用户**。如果写入失败，会返回一个错误。生产者在收到错误之后会尝试重新发送消息，几次之后如果还是失败的话，就返回错误消息。

### 2.2、消费者概述

​    应用程序使用 KafkaConsumer 从 Kafka 中订阅主题并接收来自这些主题的消息，然后再把他们保存起来。应用程序首先需要创建一个 KafkaConsumer 对象，订阅主题并开始接受消息，验证消息并保存结果。一段时间后，生产者往主题写入的速度超过了应用程序验证数据的速度，这时候该如何处理？如果只使用单个消费者的话，应用程序会跟不上消息生成的速度，就像多个生产者像相同的主题写入消息一样，这时候就需要多个消费者共同参与消费主题中的消息，对消息进行分流处理。

​     Kafka 消费者从属于消费者群组。**一个群组中的消费者订阅的都是相同的主题**，**每个消费者接收主题一部分分区的消息**。下面是一个 Kafka 分区消费示意图。

![]({{ site.url }}/assets/img/大数据/1.4.jpg)


 上图中的主题 T1 有四个分区，分别是分区0、分区1、分区2、分区3，我们创建一个消费者群组1，消费者群组中只有一个消费者，它订阅主题T1，接收到 T1 中的全部消息。由于一个消费者处理四个生产者发送到分区的消息，压力有些大，需要帮手来帮忙分担任务，于是就演变为下图 

![]({{ site.url }}/assets/img/大数据/1.5.jpg)


 如上图所示，每个分区所产生的消息能够被每个消费者群组中的消费者消费，如果向消费者群组中增加更多的消费者，那么多余的消费者将会闲置，如下图所示 

![]({{ site.url }}/assets/img/大数据/1.6.jpg)


​     **Kafka一个很重要的特性就是，只需写入一次消息，可以支持任意多的应用读取这个消息**。换句话说，每个应用都可以读到全量的消息。为了使得每个应用都能读到全量消息，应用需要有不同的消费组。对于上面的例子，假如我们新增了一个新的消费组 G2，而这个消费组有两个消费者，那么就演变为下图这样 

![]({{ site.url }}/assets/img/大数据/1.7.jpg)


​    在这个场景中，消费组 G1 和消费组 G2 都能收到 T1 主题的全量消息，在逻辑意义上来说它们属于不同的应用。

​     **总结起来就是如果应用需要读取全量消息，那么请为该应用设置一个消费组；如果该应用消费能力不足，那么可以考虑在这个消费组里增加消费者**

## 3、主题与分区

![]({{ site.url }}/assets/img/大数据/1.8.png)


​    Kafka 中，**消息以 Topic 为单位进行归类**，Producer 将消息发送到特定的 Topic 上，而 Consumer 则在启动时需要订阅某个主题并进行消费。
​    **Topic 是由若干个分区组成的，每个分区都只能属于单个的主题，事实上，Topic 只是逻辑上的概念，而分区才是 Topic 借以实现的实体。**
​    每个分区可以看作是一个可追加的日志文件，每个消息都拥有自己在分区中的偏移量 offset，而在每个分区中，消息的 offset 就成为了消息的唯一标识，依赖 offset，kafka 实现了在单个分区内的消息有序性，可以理解，**一个分区中若干条消息的消费是按照消息的 offset 有序的，而在一个 Topic 中，消息的消费是无序的**。 

## 4、Kafka如何存储数据

​       **您首先应该知道 Kafka 的消息是存在于文件系统之上的**。Kafka 高度依赖文件系统来存储和缓存消息，一般的人认为 “磁盘是缓慢的”，所以对这样的设计持有怀疑态度。实际上，磁盘比人们预想的快很多也慢很多，这取决于它们如何被使用；一个好的磁盘结构设计可以使之跟网络速度一样快。 

​    **上述的 Topic 其实是逻辑上的概念，面相消费者和生产者，物理上存储的其实是 Partition**，**每一个 Partition 最终对应一个目录，里面存储所有的消息和索引文件**。默认情况下，每一个 Topic 在创建时如果不指定 Partition 数量时只会创建 1 个 Partition。比如，我创建了一个 Topic 名字为 test ，没有指定 Partition 的数量，那么会默认创建一个 test-0 的文件夹，这里的命名规则是：`-`。 

![]({{ site.url }}/assets/img/大数据/1.9.png)


## 5、Kafka高级知识

### 5.1、Kafka如何判断一个节点还活着

1. 节点必须可以维护和Zookeeper的连接，Zookeeper通过心跳机制检查每个节点的连接。
2. 如果节点是个Follower，他必须能及时的同步leader的写操作，延时不能太久。

### 5.2、Kafka的再平衡机制

​    所谓的再平衡，指的是在kafka consumer所订阅的topic发生变化时发生的一种分区重分配机制。一般有三种情况会触发再平衡： 

- consumer group中的新增或删除某个consumer，导致其所消费的分区需要分配到组内其他的consumer上；
- consumer订阅的topic发生变化，比如订阅的topic采用的是正则表达式的形式，如`test-*`此时如果有一个新建了一个topic `test-user`，那么这个topic的所有分区也是会自动分配给当前的consumer的，此时就会发生再平衡；
- consumer所订阅的topic发生了新增分区的行为，那么新增的分区就会分配给当前的consumer，此时就会触发再平衡。

  Kafka提供的再平衡策略主要有三种：`Round Robin`，`Range`和`Sticky`，默认使用的是`Range`。这三种分配策略的主要区别在于：

- Round Robin：会采用轮询的方式将当前所有的分区依次分配给所有的consumer；
- Range：首先会计算每个consumer可以消费的分区个数，然后按照顺序将指定个数范围的分区分配给各个consumer；
- Sticky：这种分区策略是最新版本中新增的一种策略，其主要实现了两个目的：
   1、将现有的分区尽可能均衡的分配给各个consumer，存在此目的的原因在于Round Robin和Range分配策略实际上都会导致某几个consumer承载过多的分区，从而导致消费压力不均衡；
   2、如果发生再平衡，那么重新分配之后在前一点的基础上会尽力保证当前未宕机的consumer所消费的分区不会被分配给其他的consumer上；

## 6、参考资料

1、 《Kafka【入门】看这一篇就够了!》

2、《学习 Kafka 入门知识看这一篇就够了！（万字长文）》



